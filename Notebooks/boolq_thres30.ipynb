{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "F0O3lRcGFOpK",
        "outputId": "4c99afe4-4e61-46d6-9909-7d0ee2f888cd"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets==1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "QMUKOpC6Jui_",
        "outputId": "66f4bade-5742-4d13-ecdf-77cdf9153bc6"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIyG4YScJulc",
        "outputId": "301934c6-0fe6-49c0-e252-a182ce63ae58"
      },
      "outputs": [],
      "source": [
        "#!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlzmvHmIJuoN",
        "outputId": "c333c9d6-074b-4e6d-93d1-3de184fa9bba"
      },
      "outputs": [],
      "source": [
        "#!python -m pip install -U nn_pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFvKmPljFJwy",
        "outputId": "8d35643d-fee3-4231-d642-15c682b9ff0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using transformers v4.35.0 and datasets v1.4.1 and torch v2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "datasets.logging.set_verbosity_error()\n",
        "transformers.logging.set_verbosity_error()\n",
        "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__} and torch v{torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWo5RBc9FJw0",
        "outputId": "6f59758e-c101-427a-e14d-4ff92b23b03b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'label'],\n",
              "        num_rows: 9427\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'label'],\n",
              "        num_rows: 3270\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'label'],\n",
              "        num_rows: 3245\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "boolq = load_dataset(\"super_glue\", \"boolq\")\n",
        "boolq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDQRgXyiFJw0",
        "outputId": "0f20372f-ecf1-4393-94b1-31a9da602128"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'idx': 0,\n",
              " 'label': 1,\n",
              " 'passage': 'Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.',\n",
              " 'question': 'do iran and afghanistan speak the same language'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boolq['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfoePaCyFJw1",
        "outputId": "39d75c86-85a6-409e-9d7d-693226656810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'labels'],\n",
              "        num_rows: 9427\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'labels'],\n",
              "        num_rows: 3270\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'labels'],\n",
              "        num_rows: 3245\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boolq.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install --upgrade --quiet jupyter_client ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O5tU4w4lFJw1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l--secK5FJw2"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_encode(examples):\n",
        "    return tokenizer(examples['question'], examples['passage'], truncation=\"only_second\")\n",
        "\n",
        "boolq_enc = boolq.map(tokenize_and_encode, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nn_pruning in /home/ubuntu/.local/lib/python3.8/site-packages (0.1.2)\n",
            "Requirement already satisfied: torch>=1.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from nn_pruning) (2.1.0)\n",
            "Requirement already satisfied: transformers>=4.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from nn_pruning) (4.35.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /home/ubuntu/.local/lib/python3.8/site-packages (from nn_pruning) (1.3.2)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nn_pruning) (7.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (2.18.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (10.3.2.106)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (2023.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.6->nn_pruning) (2.10.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.0.106)\n",
            "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (4.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=4.3.0->nn_pruning) (5.3.1)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (4.49.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers>=4.3.0->nn_pruning) (2.22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (2023.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (0.18.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->nn_pruning) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->nn_pruning) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->nn_pruning) (1.10.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.6->nn_pruning) (12.3.52)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from sympy->torch>=1.6->nn_pruning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nn_pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nn_pruning.sparse_trainer import SparseTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "01_sparse_trainer_rohan.ipynb  checkpoints  sst_thres30.ipynb\n",
            "boolq.ipynb\t\t       models\n",
            "boolq_thres30.ipynb\t       sst.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///home/ubuntu/pruning\n",
            "\u001b[31mERROR: file:///home/ubuntu/pruning does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -m pip install -e \".[dev]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "abjwOd00FJw3"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from nn_pruning.sparse_trainer import SparseTrainer\n",
        "\n",
        "class PruningTrainer(SparseTrainer, Trainer):\n",
        "    def __init__(self, sparse_args, *args, **kwargs):\n",
        "        Trainer.__init__(self, *args, **kwargs)\n",
        "        SparseTrainer.__init__(self, sparse_args)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        outputs = model(**inputs)\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = outputs[self.args.past_index]\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "        self.metrics[\"ce_loss\"] += float(loss)\n",
        "        self.loss_counter += 1\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F5EG6z6FJw3",
        "outputId": "820c8618-78ea-4eac-9e85-ad4c8c1b627c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTrainingArguments(mask_scores_learning_rate=0.01, dense_pruning_method='topK', attention_pruning_method='topK', ampere_pruning_method='disabled', attention_output_with_dense=True, bias_mask=True, mask_init='constant', mask_scale=0.0, dense_block_rows=1, dense_block_cols=1, attention_block_rows=1, attention_block_cols=1, initial_threshold=1.0, final_threshold=0.5, initial_warmup=1, final_warmup=2, initial_ampere_temperature=0.0, final_ampere_temperature=20.0, regularization='disabled', regularization_final_lambda=0.0, attention_lambda=1.0, dense_lambda=1.0, distil_teacher_name_or_path=None, distil_alpha_ce=0.5, distil_alpha_teacher=0.5, distil_temperature=2.0, final_finetune=False, layer_norm_patch=False, layer_norm_patch_steps=50000, layer_norm_patch_start_delta=0.99, gelu_patch=False, gelu_patch_steps=50000, linear_min_parameters=0.005, rewind_model_name_or_path=None)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nn_pruning.patch_coordinator import SparseTrainingArguments\n",
        "\n",
        "sparse_args = SparseTrainingArguments()\n",
        "sparse_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g3O9wdNNFJw5"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    \"dense_pruning_method\": \"topK:1d_alt\",\n",
        "    \"attention_pruning_method\": \"topK\",\n",
        "    \"initial_threshold\": 1.0,\n",
        "    \"final_threshold\": 0.3,\n",
        "    \"initial_warmup\": 1,\n",
        "    \"final_warmup\": 3,\n",
        "    \"attention_block_rows\":32,\n",
        "    \"attention_block_cols\":32,\n",
        "    \"attention_output_with_dense\": 0\n",
        "}\n",
        "\n",
        "for k,v in hyperparams.items():\n",
        "    if hasattr(sparse_args, k):\n",
        "        setattr(sparse_args, k, v)\n",
        "    else:\n",
        "        print(f\"sparse_args does not have argument {k}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BPI7xpfyFJw6"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "num_train_epochs = 6\n",
        "logging_steps = len(boolq_enc[\"train\"]) // batch_size\n",
        "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=logging_steps,\n",
        "    save_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BVD7J0bcFJw7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from nn_pruning.patch_coordinator import ModelPatchingCoordinator\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "mpc = ModelPatchingCoordinator(\n",
        "    sparse_args=sparse_args,\n",
        "    device=device,\n",
        "    cache_dir=\"checkpoints\",\n",
        "    logit_names=\"logits\",\n",
        "    teacher_constructor=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PKGocDr_FJw8"
      },
      "outputs": [],
      "source": [
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_ckpt).to(device)\n",
        "mpc.patch_model(bert_model)\n",
        "\n",
        "bert_model.save_pretrained(\"models/patched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LA6AdtV_FJw9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "accuracy_score = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy_score.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VuzyDBphFJw9"
      },
      "outputs": [],
      "source": [
        "trainer = PruningTrainer(\n",
        "    sparse_args=sparse_args,\n",
        "    args=args,\n",
        "    model=bert_model,\n",
        "    train_dataset=boolq_enc[\"train\"],\n",
        "    eval_dataset=boolq_enc[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZH6qUoQyFJw9"
      },
      "outputs": [],
      "source": [
        "trainer.set_patch_coordinator(mpc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmaiTgXFJw9"
      },
      "source": [
        "and fine-prune:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "p8GwLNOJFJw9",
        "outputId": "f4ebdd81-1f54-4f1d-d15e-e10e2af16843"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3540' max='3540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3540/3540 52:56, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Regu Lambda</th>\n",
              "      <th>Ampere Temperature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.661300</td>\n",
              "      <td>0.670379</td>\n",
              "      <td>0.621713</td>\n",
              "      <td>63.989600</td>\n",
              "      <td>51.102000</td>\n",
              "      <td>3.204000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.620600</td>\n",
              "      <td>0.660605</td>\n",
              "      <td>0.621713</td>\n",
              "      <td>63.740100</td>\n",
              "      <td>51.302000</td>\n",
              "      <td>3.216000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.559000</td>\n",
              "      <td>0.635593</td>\n",
              "      <td>0.633333</td>\n",
              "      <td>63.719400</td>\n",
              "      <td>51.319000</td>\n",
              "      <td>3.217000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.488900</td>\n",
              "      <td>0.593086</td>\n",
              "      <td>0.688991</td>\n",
              "      <td>63.687700</td>\n",
              "      <td>51.344000</td>\n",
              "      <td>3.219000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.421000</td>\n",
              "      <td>0.717192</td>\n",
              "      <td>0.661468</td>\n",
              "      <td>63.710000</td>\n",
              "      <td>51.326000</td>\n",
              "      <td>3.218000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.365400</td>\n",
              "      <td>0.736642</td>\n",
              "      <td>0.678899</td>\n",
              "      <td>63.819600</td>\n",
              "      <td>51.238000</td>\n",
              "      <td>3.212000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MfWZaDkgFJw-"
      },
      "outputs": [],
      "source": [
        "output_model_path = \"models/bert-base-uncased-finepruned-boolq-less\"\n",
        "trainer.save_model(output_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cIU7rtq9FJw-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11, 144)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mpc.compile_model(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (4.44.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/ubuntu/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Yr_AVcrcFJw_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "removed heads 0, total_heads=133, percentage removed=0.0\n",
            "bert.encoder.layer.0.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.0.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.1.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.1.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.2.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.2.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.3.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.3.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.4.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.4.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.5.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.5.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.6.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.6.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.7.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.7.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.8.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.8.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.9.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.9.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.10.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.10.output.dense, sparsity = 69.99\n",
            "bert.encoder.layer.11.intermediate.dense, sparsity = 69.99\n",
            "bert.encoder.layer.11.output.dense, sparsity = 69.99\n"
          ]
        }
      ],
      "source": [
        "from nn_pruning.inference_model_patcher import optimize_model\n",
        "\n",
        "prunebert_model = optimize_model(trainer.model, \"dense\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BxZT0AnFJw_"
      },
      "source": [
        "We can also see what fraction of total parameters remain in our pruned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "107318978"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67664378"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prunebert_model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "001QJCBpFJw_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6304977857690743"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prunebert_model.num_parameters() / bert_model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VYPA85cNFJw_"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "def compute_latencies(model,\n",
        "                      question=\"Is Saving Private Ryan based on a book?\",\n",
        "                      passage=\"\"\"In 1994, Robert Rodat wrote the script for the film. Rodat’s script was submitted to\n",
        "                      producer Mark Gordon, who liked it and in turn passed it along to Spielberg to direct. The film is\n",
        "                      loosely based on the World War II life stories of the Niland brothers. A shooting date was set for\n",
        "                      June 27, 1997\"\"\"):\n",
        "    inputs = tokenizer(question, passage, truncation=\"only_second\", return_tensors=\"pt\")\n",
        "    latencies = []\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    for _ in range(100):\n",
        "        start_time = perf_counter()\n",
        "        _ = model(**inputs)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "        # Compute run statistics\n",
        "        time_avg_ms = 1000 * np.mean(latencies)\n",
        "        time_std_ms = 1000 * np.std(latencies)\n",
        "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "i8vMxCmAFJw_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average latency (ms) - 60.76 +\\- 0.14\n"
          ]
        }
      ],
      "source": [
        "latencies = {}\n",
        "latencies[\"prunebert\"] = compute_latencies(prunebert_model.to(\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wTYSrpqyFJxA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average latency (ms) - 104.06 +\\- 3.07\n"
          ]
        }
      ],
      "source": [
        "bert_unpruned = AutoModelForSequenceClassification.from_pretrained(\"lewtun/bert-base-uncased-finetuned-boolq\").to(\"cpu\")\n",
        "\n",
        "latencies[\"bert-base\"] = compute_latencies(bert_unpruned.to(\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ref: https://github.com/huggingface/nn_pruning"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
