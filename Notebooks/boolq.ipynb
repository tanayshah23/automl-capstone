{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "F0O3lRcGFOpK",
        "outputId": "4c99afe4-4e61-46d6-9909-7d0ee2f888cd"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets==1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "QMUKOpC6Jui_",
        "outputId": "66f4bade-5742-4d13-ecdf-77cdf9153bc6"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIyG4YScJulc",
        "outputId": "301934c6-0fe6-49c0-e252-a182ce63ae58"
      },
      "outputs": [],
      "source": [
        "#!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlzmvHmIJuoN",
        "outputId": "c333c9d6-074b-4e6d-93d1-3de184fa9bba"
      },
      "outputs": [],
      "source": [
        "#!python -m pip install -U nn_pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFvKmPljFJwy",
        "outputId": "8d35643d-fee3-4231-d642-15c682b9ff0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using transformers v4.35.0 and datasets v1.4.1 and torch v2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "datasets.logging.set_verbosity_error()\n",
        "transformers.logging.set_verbosity_error()\n",
        "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__} and torch v{torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWo5RBc9FJw0",
        "outputId": "6f59758e-c101-427a-e14d-4ff92b23b03b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'label'],\n",
              "        num_rows: 9427\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'label'],\n",
              "        num_rows: 3270\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'label'],\n",
              "        num_rows: 3245\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "boolq = load_dataset(\"super_glue\", \"boolq\")\n",
        "boolq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDQRgXyiFJw0",
        "outputId": "0f20372f-ecf1-4393-94b1-31a9da602128"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'idx': 0,\n",
              " 'label': 1,\n",
              " 'passage': 'Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.',\n",
              " 'question': 'do iran and afghanistan speak the same language'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boolq['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfoePaCyFJw1",
        "outputId": "39d75c86-85a6-409e-9d7d-693226656810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'labels'],\n",
              "        num_rows: 9427\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'labels'],\n",
              "        num_rows: 3270\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'passage', 'idx', 'labels'],\n",
              "        num_rows: 3245\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boolq.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install --upgrade --quiet jupyter_client ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O5tU4w4lFJw1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l--secK5FJw2"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_encode(examples):\n",
        "    return tokenizer(examples['question'], examples['passage'], truncation=\"only_second\")\n",
        "\n",
        "boolq_enc = boolq.map(tokenize_and_encode, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nn_pruning\n",
            "  Using cached nn_pruning-0.1.2-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nn_pruning) (7.0)\n",
            "Requirement already satisfied: torch>=1.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from nn_pruning) (2.1.0)\n",
            "Requirement already satisfied: transformers>=4.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from nn_pruning) (4.35.0)\n",
            "Collecting scikit-learn>=0.24\n",
            "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 5.9 MB/s eta 0:00:01     |██████████████████████████████▋ | 10.6 MB 5.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (4.8.0)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (3.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.6->nn_pruning) (2.10.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.0.106)\n",
            "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (3.1)\n",
            "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (1.12)\n",
            "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (2.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.6->nn_pruning) (11.0.2.54)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (4.49.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers>=4.3.0->nn_pruning) (2.22.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (23.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (0.18.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=4.3.0->nn_pruning) (5.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (2023.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers>=4.3.0->nn_pruning) (1.24.4)\n",
            "Collecting scipy>=1.5.0\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 99.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.6->nn_pruning) (12.3.52)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from sympy->torch>=1.6->nn_pruning) (1.3.0)\n",
            "Installing collected packages: scipy, threadpoolctl, joblib, scikit-learn, nn-pruning\n",
            "Successfully installed joblib-1.3.2 nn-pruning-0.1.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nn_pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nn_pruning.sparse_trainer import SparseTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "01_sparse_trainer.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///home/ubuntu/pruning/nn_pruning\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nn-pruning==0.1.2) (8.1.7)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /opt/conda/lib/python3.10/site-packages (from nn-pruning==0.1.2) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from nn-pruning==0.1.2) (2.1.0)\n",
            "Requirement already satisfied: transformers>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from nn-pruning==0.1.2) (4.35.0)\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.4.3-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.1/325.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24->nn-pruning==0.1.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24->nn-pruning==0.1.2) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24->nn-pruning==0.1.2) (1.11.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24->nn-pruning==0.1.2) (1.26.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (10.3.2.106)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (2.18.1)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (1.12)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (8.9.2.26)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (4.8.0)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (12.1.0.106)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (12.1.105)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (3.13.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->nn-pruning==0.1.2) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->nn-pruning==0.1.2) (12.3.52)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (2023.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (0.17.3)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (2.28.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.3.0->nn-pruning==0.1.2) (6.0.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest->nn-pruning==0.1.2) (1.0.0)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Collecting tomli>=1.0.0\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->nn-pruning==0.1.2) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.3.0->nn-pruning==0.1.2) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.3.0->nn-pruning==0.1.2) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.3.0->nn-pruning==0.1.2) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.3.0->nn-pruning==0.1.2) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->nn-pruning==0.1.2) (1.3.0)\n",
            "Building wheels for collected packages: nn-pruning\n",
            "  Building editable for nn-pruning (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for nn-pruning: filename=nn_pruning-0.1.2-0.editable-py3-none-any.whl size=7289 sha256=34d09fcb65de01234aa971e295274315385fc10eb7ecd5e78b83db290daddb4b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3pzs4g3d/wheels/eb/ac/7c/322bd7d9e98ec97184843f6271bce1f038c7c728fd170c80ee\n",
            "Successfully built nn-pruning\n",
            "Installing collected packages: tomli, iniconfig, exceptiongroup, pytest, nn-pruning\n",
            "  Attempting uninstall: nn-pruning\n",
            "    Found existing installation: nn-pruning 0.1.2\n",
            "    Uninstalling nn-pruning-0.1.2:\n",
            "      Successfully uninstalled nn-pruning-0.1.2\n",
            "Successfully installed exceptiongroup-1.1.3 iniconfig-2.0.0 nn-pruning-0.1.2 pytest-7.4.3 tomli-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -e \".[dev]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "abjwOd00FJw3"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from nn_pruning.sparse_trainer import SparseTrainer\n",
        "\n",
        "class PruningTrainer(SparseTrainer, Trainer):\n",
        "    def __init__(self, sparse_args, *args, **kwargs):\n",
        "        Trainer.__init__(self, *args, **kwargs)\n",
        "        SparseTrainer.__init__(self, sparse_args)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        outputs = model(**inputs)\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = outputs[self.args.past_index]\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "        self.metrics[\"ce_loss\"] += float(loss)\n",
        "        self.loss_counter += 1\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F5EG6z6FJw3",
        "outputId": "820c8618-78ea-4eac-9e85-ad4c8c1b627c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTrainingArguments(mask_scores_learning_rate=0.01, dense_pruning_method='topK', attention_pruning_method='topK', ampere_pruning_method='disabled', attention_output_with_dense=True, bias_mask=True, mask_init='constant', mask_scale=0.0, dense_block_rows=1, dense_block_cols=1, attention_block_rows=1, attention_block_cols=1, initial_threshold=1.0, final_threshold=0.5, initial_warmup=1, final_warmup=2, initial_ampere_temperature=0.0, final_ampere_temperature=20.0, regularization='disabled', regularization_final_lambda=0.0, attention_lambda=1.0, dense_lambda=1.0, distil_teacher_name_or_path=None, distil_alpha_ce=0.5, distil_alpha_teacher=0.5, distil_temperature=2.0, final_finetune=False, layer_norm_patch=False, layer_norm_patch_steps=50000, layer_norm_patch_start_delta=0.99, gelu_patch=False, gelu_patch_steps=50000, linear_min_parameters=0.005, rewind_model_name_or_path=None)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nn_pruning.patch_coordinator import SparseTrainingArguments\n",
        "\n",
        "sparse_args = SparseTrainingArguments()\n",
        "sparse_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g3O9wdNNFJw5"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    \"dense_pruning_method\": \"topK:1d_alt\",\n",
        "    \"attention_pruning_method\": \"topK\",\n",
        "    \"initial_threshold\": 1.0,\n",
        "    \"final_threshold\": 0.5,\n",
        "    \"initial_warmup\": 1,\n",
        "    \"final_warmup\": 3,\n",
        "    \"attention_block_rows\":32,\n",
        "    \"attention_block_cols\":32,\n",
        "    \"attention_output_with_dense\": 0\n",
        "}\n",
        "\n",
        "for k,v in hyperparams.items():\n",
        "    if hasattr(sparse_args, k):\n",
        "        setattr(sparse_args, k, v)\n",
        "    else:\n",
        "        print(f\"sparse_args does not have argument {k}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BPI7xpfyFJw6"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "num_train_epochs = 6\n",
        "logging_steps = len(boolq_enc[\"train\"]) // batch_size\n",
        "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=logging_steps,\n",
        "    save_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BVD7J0bcFJw7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from nn_pruning.patch_coordinator import ModelPatchingCoordinator\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "mpc = ModelPatchingCoordinator(\n",
        "    sparse_args=sparse_args,\n",
        "    device=device,\n",
        "    cache_dir=\"checkpoints\",\n",
        "    logit_names=\"logits\",\n",
        "    teacher_constructor=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PKGocDr_FJw8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37774972189a48c98c949f632b56cc74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='model.safetensors', max=440449768.0, style=ProgressStyle(…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_ckpt).to(device)\n",
        "mpc.patch_model(bert_model)\n",
        "\n",
        "bert_model.save_pretrained(\"models/patched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LA6AdtV_FJw9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "accuracy_score = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy_score.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VuzyDBphFJw9"
      },
      "outputs": [],
      "source": [
        "trainer = PruningTrainer(\n",
        "    sparse_args=sparse_args,\n",
        "    args=args,\n",
        "    model=bert_model,\n",
        "    train_dataset=boolq_enc[\"train\"],\n",
        "    eval_dataset=boolq_enc[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZH6qUoQyFJw9"
      },
      "outputs": [],
      "source": [
        "trainer.set_patch_coordinator(mpc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "p8GwLNOJFJw9",
        "outputId": "f4ebdd81-1f54-4f1d-d15e-e10e2af16843"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3540' max='3540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3540/3540 52:47, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Regu Lambda</th>\n",
              "      <th>Ampere Temperature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.657400</td>\n",
              "      <td>0.664053</td>\n",
              "      <td>0.622018</td>\n",
              "      <td>64.673700</td>\n",
              "      <td>50.562000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.603000</td>\n",
              "      <td>0.630869</td>\n",
              "      <td>0.638226</td>\n",
              "      <td>64.703100</td>\n",
              "      <td>50.539000</td>\n",
              "      <td>3.168000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.516800</td>\n",
              "      <td>0.596548</td>\n",
              "      <td>0.687156</td>\n",
              "      <td>64.674100</td>\n",
              "      <td>50.561000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.429000</td>\n",
              "      <td>0.639322</td>\n",
              "      <td>0.680428</td>\n",
              "      <td>64.649700</td>\n",
              "      <td>50.580000</td>\n",
              "      <td>3.171000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.344700</td>\n",
              "      <td>0.815360</td>\n",
              "      <td>0.688073</td>\n",
              "      <td>64.655900</td>\n",
              "      <td>50.575000</td>\n",
              "      <td>3.171000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.276800</td>\n",
              "      <td>0.837686</td>\n",
              "      <td>0.678899</td>\n",
              "      <td>64.599500</td>\n",
              "      <td>50.620000</td>\n",
              "      <td>3.173000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MfWZaDkgFJw-"
      },
      "outputs": [],
      "source": [
        "output_model_path = \"models/bert-base-uncased-finepruned-boolq\"\n",
        "trainer.save_model(output_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cIU7rtq9FJw-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 144)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mpc.compile_model(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Yr_AVcrcFJw_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "removed heads 0, total_heads=144, percentage removed=0.0\n",
            "bert.encoder.layer.0.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.0.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.1.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.1.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.2.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.2.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.3.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.3.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.4.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.4.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.5.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.5.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.6.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.6.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.7.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.7.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.8.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.8.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.9.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.9.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.10.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.10.output.dense, sparsity = 50.00\n",
            "bert.encoder.layer.11.intermediate.dense, sparsity = 50.00\n",
            "bert.encoder.layer.11.output.dense, sparsity = 50.00\n"
          ]
        }
      ],
      "source": [
        "from nn_pruning.inference_model_patcher import optimize_model\n",
        "\n",
        "prunebert_model = optimize_model(trainer.model, \"dense\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "001QJCBpFJw_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7412403506937804"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prunebert_model.num_parameters() / bert_model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VYPA85cNFJw_"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "def compute_latencies(model,\n",
        "                      question=\"Is Saving Private Ryan based on a book?\",\n",
        "                      passage=\"\"\"In 1994, Robert Rodat wrote the script for the film. Rodat’s script was submitted to\n",
        "                      producer Mark Gordon, who liked it and in turn passed it along to Spielberg to direct. The film is\n",
        "                      loosely based on the World War II life stories of the Niland brothers. A shooting date was set for\n",
        "                      June 27, 1997\"\"\"):\n",
        "    inputs = tokenizer(question, passage, truncation=\"only_second\", return_tensors=\"pt\")\n",
        "    latencies = []\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    for _ in range(100):\n",
        "        start_time = perf_counter()\n",
        "        _ = model(**inputs)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "        # Compute run statistics\n",
        "        time_avg_ms = 1000 * np.mean(latencies)\n",
        "        time_std_ms = 1000 * np.std(latencies)\n",
        "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "i8vMxCmAFJw_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average latency (ms) - 61.33 +\\- 0.31\n"
          ]
        }
      ],
      "source": [
        "latencies = {}\n",
        "latencies[\"prunebert\"] = compute_latencies(prunebert_model.to(\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wTYSrpqyFJxA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef846d00d77849ee98cda94f35e87706",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='(…)finetuned-boolq/resolve/main/config.json', max=563.0, …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "685142a74f7148b79d57521f5dd2340b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='pytorch_model.bin', max=438022420.0, style=ProgressStyle(…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average latency (ms) - 85.28 +\\- 0.61\n"
          ]
        }
      ],
      "source": [
        "bert_unpruned = AutoModelForSequenceClassification.from_pretrained(\"lewtun/bert-base-uncased-finetuned-boolq\").to(\"cpu\")\n",
        "\n",
        "latencies[\"bert-base\"] = compute_latencies(bert_unpruned.to(\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ref: https://github.com/huggingface/nn_pruning"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
